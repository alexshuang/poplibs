#!/usr/bin/env python3
# Copyright (c) 2020 Graphcore Ltd. All rights reserved.

"""
A tool to run all poplibs benchmarks and update expected results.
"""

import argparse
import collections
import subprocess
import csv
import os
import sys
import tempfile
import re
from progress.bar import Bar
from bench import read_results_file, CHANGED_RESULT_PATTERN, TestKey, Expected

BENCHMARK_RESULTS_NOTICE="""\
# This file is automatically generated and updated by update_bench.py
# Do not modify this by hand.
# Target,Config,Name,Cycles,CycleChange(%),TotalMemory,TotalMemoryChange(%),MaxTileMemory,MaxTileMemoryChange(%)
"""

RUNNING_PROGRESS = re.compile('^\s+Start (\d+): ([a-zA-Z]+)(2?)_default_(.+)_benchmark$')

TOOLS_DIR=os.path.dirname(os.path.realpath(__file__))
DEFAULT_CSV=TOOLS_DIR + '/../tests/benchmark_results.csv'

class BenchmarksBar(Bar):
    suffix = '%(index)d/%(max)d - %(elapsed)ds'

def write_results(out_path, expected):
    with open(out_path, 'w') as results_file:
        results_file.write(BENCHMARK_RESULTS_NOTICE)
        results_writer = csv.writer(results_file,
                                    delimiter=',',
                                    lineterminator=os.linesep)
        for test_key in sorted(expected.keys()):
            entry = expected[test_key]
            results_writer.writerow(
                [test_key.target, test_key.config, test_key.name,
                 int(entry.cycles), entry.cycles_change, 
                 int(entry.total_memory), entry.total_memory_change, 
                 int(entry.max_tile_memory), entry.max_tile_memory_change])

def updated_results_iter(cmd, number_of_benchmarks):
    testsCount = 1
    print('Collecting updates with command: ', *cmd)
    proc=subprocess.Popen(cmd, stdout=subprocess.PIPE)
    with BenchmarksBar('Running', max=number_of_benchmarks) as bar:
        for line in proc.stdout:
            dline = line.decode('utf-8')
            progressMatch = RUNNING_PROGRESS.match(dline)
            if progressMatch:
                testsCount = testsCount + 1
                bar.next()

            match = CHANGED_RESULT_PATTERN.match(dline)
            if match:
                result = match.groups()
                yield (TestKey._make(result[0:3]), Expected._make(result[3:]))
            
def main():
    parser = argparse.ArgumentParser(description="Benchmark results updater")
    parser.add_argument("test_script", help='Path to test.sh script.')
    parser.add_argument(
        "--expected_csv", nargs='?', default=DEFAULT_CSV,
        help='Path to a file containing csv with expected results for '
             'benchmarks. Defaults tests/benchmark_results.csv',
    )
    args = parser.parse_args()

    expected_dict = read_results_file(args.expected_csv)
    number_of_benchmarks = len(expected_dict)

    nproc = os.cpu_count()
    cmd = [args.test_script, 'poplibs', '-L', 'benchmarks',
           '--output-on-failure', f'-j{nproc}']
    num_updates = 0
    max_increase = [0, 0, 0]
    update_report = ""
    for test_key, actual in updated_results_iter(cmd, number_of_benchmarks):
        actual = Expected._make(map(float, actual))
        expected = expected_dict[test_key]
        increase = []
        for field in actual._fields:
          if field.endswith('_change'):
            increase.append(getattr(actual, field))
        max_increase = max(max_increase, increase)
        expected_dict[test_key] = actual
        update_report += f'Updating {test_key} with results {actual}'
        # Write out the CSV on every update to ease inspection while a long
        # update is executing
        write_results(args.expected_csv, expected_dict)
        num_updates += 1

    if num_updates > 0:
        print(f'Highest increases: {max_increase}')
    print(f'Done. Updated {num_updates} benchmark results')
    print(update_report)

if __name__ == "__main__":
    main()
